# write-you-a-lexer

Write-You-A-Lexer is a project that implements a lexer for Python 3.9 version. This project has five `Example` folders. Each `Example` folder builds on the previous version. But, each `Example` folder can be run and tested indepenently.

This project is used in the tutorial `Alex By Example (Write-You-A-Python-Lexer) and it can be understood by going through the tutorial.

# Layout of the Project

The project contains five `Example` folders.  Each `Example` folder contains the following modules.

 - Lexer.x - Contains the implementation of lexer features explained under each section in the tutorial.
 - LexerRunner.x - Wrapper for running the `Lexer` under the respective `Example` folder.
 - LexerUtil.x - All the helper function/definitions required by the `Lexer.x`. The `Lexer.x` imports this module.
 - Tokens.hs   - Defintions of `Tokens` the Lexer generates.

All modules are named the same across all Example folders. Therfore, the above description holds true for all `Example` folders.

 ## Tests

 For each `Example` folder we have a corresponding tests under `test_tokenizer<no>.hs`, where `<no>` is the `Example` number. For examle, all tests for `Example2` lexer can be run by running the tests in `test_tokenizer2.hs`. The tests uses input data from test_fixtures files. All the test fixtures, for each `Example` folder is available under `test/test_fixtures/<no>`


``` bash
$ stack test --test-arguments "--pattern=tokenizer_tests_example_1"  wya-lexer:wya-lexer-test-1
wya-lexer> test (suite: wya-lexer-test-1, args: --pattern=tokenizer_tests_example_1)

tokenizer_tests_example_1
  ./test/test_fixtures/1/3.txt: OK
  ./test/test_fixtures/1/2.txt: OK
  ./test/test_fixtures/1/1.txt: OK
  ./test/test_fixtures/1/3.txt: OK
  ./test/test_fixtures/1/2.txt: OK
  ./test/test_fixtures/1/1.txt: OK

All 6 tests passed (0.00s)

wya-lexer> Test suite wya-lexer-test-1 passed

```

 The `test_fixtures` (around 80 of them) were generated by capturing in the input and the results by running `Lib/test/test_tokenizer.py`. This implementation passes for all those tests. Caveat: `Unicode` support for identifiers are not supported. There may still be cases this implementation may not support. The project is still in testing phase.

## Execute

There are multiple ways to use this project.

### Running from command line

Each example can be run from the command line either by providing `input` directly at the console and providing a valid `Python` file. Note, that depending on which `Example` is run, the program could fail if the grammar is not supported. For example `Example1` lexer implementation may not support all tokens in a provided valid `Python` file. `Example5` should work on any Python file (provided there are not bugs!)

``` bash
stack exec wya-lexer-exe -- --example_name Example5 --file_name "hello_world.py"
```

Here are some examples, where the prior Example does not support a grammar but using a later version the tokens are generated.

``` bash
 ~/fsf/wya-lexer on git:main x
$ stack exec wya-lexer-exe -- --example_name Example1
identifier
42
Right [TokenInfo {token_type = Name, token_string = "identifier", start_pos = (0,0), end_pos = (0,0)},TokenInfo {token_type = Number, token_string = "42", start_pos = (0,0), end_pos = (0,0)}]

#  ~/fsf/wya-lexer on git:main x
$ stack exec wya-lexer-exe -- --example_name Example2
a + b
wya-lexer-exe: Lexical error(' ',[],"+ b\n")
CallStack (from HasCallStack):
  error, called at src/Example2/Lexer.x:103:28 in wya-lexer-0.1.0.0-CGMHpbYSL6uA5793UGrB8f:Example2.Lexer

#  ~/fsf/wya-lexer on git:main x C:1
$ stack exec wya-lexer-exe -- --example_name Example3
a + b
Right [TokenInfo {token_type = Name, token_string = "a", start_pos = (1,1), end_pos = (1,1)},TokenInfo {token_type = Plus, token_string = "+", start_pos = (1,3), end_pos = (1,3)},TokenInfo {token_type = Name, token_string = "b", start_pos = (1,5), end_pos = (1,5)}]

#  ~/fsf/wya-lexer on git:main x
$

```

# Caveats

The library is still in testing phase. This library was implemented to support the accompanying tutorial.
